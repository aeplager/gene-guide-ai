# üéØ Application Prompts Reference

This document contains all prompts and conversational contexts used in the Gene Guide AI application.

---

## üìä 1. Genetic Condition Analysis Prompt

**Location:** `app.py` (lines 852-893)  
**Endpoint:** `GET /condition-analysis/<user_id>`  
**Purpose:** Generates comprehensive genetic counseling analysis from user's genetic data  
**LLM:** Custom LLM (Azure Container Apps)  
**Model:** `custom-llm-gc`

### Full Prompt Template:

```
You are a professional genetic counselor providing educational information about genetic test results. 

Given the following genetic information:
- Gene: {gene}
- Variant/Mutation: {mutation}
- Classification: {classification}

Please provide a comprehensive analysis in the following JSON format:

{
  "condition": "Primary condition name associated with this gene variant",
  "riskLevel": "High/Moderate/Low",
  "description": "A clear, patient-friendly 2-3 sentence description of what this variant means",
  "implications": [
    "First health implication",
    "Second health implication",
    "Third health implication",
    "Fourth health implication"
  ],
  "recommendations": [
    "First recommended action",
    "Second recommended action",
    "Third recommended action",
    "Fourth recommended action"
  ],
  "resources": [
    "First educational resource name",
    "Second educational resource name",
    "Third educational resource name",
    "Fourth educational resource name"
  ]
}

Important guidelines:
- Use clear, non-technical language suitable for patients
- Base risk level on the classification: Pathogenic/Likely Pathogenic = High, VUS = Moderate, Benign/Likely Benign = Low
- Focus on actionable information
- Include both risks and positive steps they can take
- Be compassionate and supportive in tone
- Provide specific, evidence-based information

CRITICAL: Respond ONLY with the JSON object, no additional text.
```

### Example Usage:

**Input Variables:**
- `gene`: "BRCA1"
- `mutation`: "c.185delAG"
- `classification`: "Pathogenic"

**Generated Prompt:**
```
You are a professional genetic counselor providing educational information about genetic test results. 

Given the following genetic information:
- Gene: BRCA1
- Variant/Mutation: c.185delAG
- Classification: Pathogenic

Please provide a comprehensive analysis in the following JSON format:
...
```

**Expected Output:**
```json
{
  "condition": "Hereditary Breast and Ovarian Cancer Syndrome",
  "riskLevel": "High",
  "description": "This pathogenic variant in BRCA1 significantly increases your lifetime risk for breast and ovarian cancers. The c.185delAG variant is a well-documented mutation that impairs the gene's ability to repair DNA damage.",
  "implications": [
    "Up to 70% lifetime risk of breast cancer",
    "Up to 44% lifetime risk of ovarian cancer",
    "Increased risk for male breast cancer in family members",
    "Earlier onset of cancers compared to general population"
  ],
  "recommendations": [
    "Enhanced breast screening with MRI and mammography starting at age 25",
    "Consider risk-reducing surgeries after completing family planning",
    "Genetic counseling for family members who may carry the variant",
    "Regular gynecologic surveillance and consider preventive oophorectomy"
  ],
  "resources": [
    "FORCE (Facing Our Risk of Cancer Empowered)",
    "National Cancer Institute - BRCA Gene Mutations",
    "Bright Pink - High-Risk Breast Cancer Support",
    "NCCN Guidelines for Genetic/Familial High-Risk Assessment"
  ]
}
```

---

## üí¨ 2. Tavus Video Conversation (Optional Context)

**Location:** Currently NOT implemented in active `app.py`  
**Status:** Can be added via `conversational_context` field  
**Purpose:** System instructions for the AI genetics counselor in video conversations  

### Current Implementation:

The current `app.py` does NOT include a `conversational_context` field in the Tavus API request. This means Tavus uses either:
1. The default persona configuration set in Tavus Dashboard
2. The persona ID's pre-configured system prompt (`TAVUS_PERSONA_ID = p70ec11f62ec`)

### Legacy Example (from `tavus_backend.py`):

**Note:** This is from a different application ("Legacy Forever") but shows the pattern:

```python
"conversational_context": (
    "You are StorySeeker, an engaging, empathetic AI host whose job is to learn "
    "about each guest's life story, passions, and proudest achievements."
)
```

### Suggested Genetics Counselor Context:

If you want to add a custom conversational context for the Tavus video, you could add this to `app.py`:

```python
body = {
    "replica_id": TAVUS_REPLICA_ID,
    "conversation_name": conversation_name,
    "persona_id": TAVUS_PERSONA_ID,
    "properties": {"enable_closed_captions": False, "enable_recording": False},
    "conversational_context": """You are a compassionate, certified genetic counselor specializing in hereditary cancer syndromes. Your role is to:

1. Help patients understand their genetic test results in clear, non-technical language
2. Discuss health implications and cancer risk management options
3. Provide emotional support and answer questions with empathy
4. Guide patients on screening recommendations and preventive measures
5. Explain hereditary patterns and implications for family members

Key principles:
- Use patient-friendly language, avoid jargon
- Be supportive and non-directive
- Acknowledge emotions (fear, uncertainty, hope)
- Focus on actionable next steps
- Respect patient autonomy in decision-making
- Maintain medical accuracy while being approachable

Current patient context: The patient has received genetic test results and is seeking guidance on what they mean and what steps to take next."""
}
```

---

## üß™ 3. Custom LLM Configuration

**Base URL:** `https://custom-llm-gc.ashydune-c5455a7b.centralus.azurecontainerapps.io`  
**Persona ID:** `9b94acf5-6fcb-4314-9049-fad8d641206d`  
**Model:** `custom-llm-gc`

### API Call Structure:

```python
url = f"{CUSTOM_LLM_BASE_URL}/v1/chat/completions"
payload = {
    "model": "custom-llm-gc",
    "conversation_id": "<uuid>",  # Random UUID per conversation
    "persona_id": CUSTOM_LLM_PERSONA_ID,
    "max_tokens": 1024,
    "stream": False,
    "messages": [
        {"role": "user", "content": prompt}
    ]
}
headers = {
    "Content-Type": "application/json",
    "x-api-key": CUSTOM_LLM_API_KEY
}
```

**Note:** The persona configuration (system prompt, behavior, RAG context) is stored in the custom LLM service itself, identified by `persona_id`. This document only covers prompts sent from the Gene Guide AI application.

---

## üìã Prompt Design Principles

### Current Approach:

1. **Structured Output**
   - JSON format for easy parsing
   - Consistent field names across responses
   - Clear schema with examples

2. **Patient-Centric Language**
   - Avoid medical jargon
   - Use clear, compassionate tone
   - Focus on actionable information

3. **Evidence-Based**
   - Risk levels tied to classification standards
   - Specific, medically accurate information
   - Evidence-based recommendations

4. **Comprehensive Coverage**
   - Health implications
   - Screening recommendations
   - Preventive measures
   - Educational resources

5. **Safety Guardrails**
   - "CRITICAL: Respond ONLY with JSON" prevents hallucination
   - Structured format ensures parseable output
   - Clear guidelines for tone and accuracy

---

## üîÑ Prompt Evolution

### Version History:

**v1.0 (Current)**
- Single prompt for condition analysis
- JSON-structured output
- Patient-friendly language focus
- 7-day caching implemented

**Potential Future Enhancements:**
- Multi-turn conversation support
- Family history integration
- Personalized risk calculation
- Treatment option comparison
- Clinical trial matching

---

## üß™ Testing Prompts

### Test with Different Genes:

**BRCA1 Pathogenic:**
```
Gene: BRCA1
Variant: c.185delAG
Classification: Pathogenic
```

**BRCA2 VUS:**
```
Gene: BRCA2
Variant: c.5266dupC
Classification: VUS (Variant of Uncertain Significance)
```

**Lynch Syndrome:**
```
Gene: MLH1
Variant: c.1852_1854del
Classification: Likely Pathogenic
```

**Benign Variant:**
```
Gene: BRCA1
Variant: c.2612C>T
Classification: Benign
```

### Expected Response Characteristics:

| Classification | Risk Level | Tone | Recommendations |
|----------------|-----------|------|-----------------|
| Pathogenic | High | Serious but supportive | Aggressive screening, preventive surgery options |
| Likely Pathogenic | High-Moderate | Cautiously serious | Enhanced screening, genetic counseling |
| VUS | Moderate | Reassuring but vigilant | Standard screening, re-evaluation over time |
| Likely Benign | Low | Reassuring | Standard screening, no special measures |
| Benign | Low | Reassuring | Standard population screening |

---

## üìù Customizing Prompts

### To Modify the Condition Analysis Prompt:

1. **Edit `app.py`** (lines 852-893)
2. **Test locally** with different genetic variants
3. **Verify JSON output** is still parseable
4. **Check cache invalidation** (may need to clear existing cache)
5. **Deploy to Azure**

### Prompt Engineering Tips:

1. **Be Specific:** Clearly define expected output format
2. **Give Examples:** Show desired structure inline
3. **Set Guardrails:** "Respond ONLY with JSON" prevents deviation
4. **Use Variables:** `{gene}`, `{mutation}`, `{classification}` for dynamic content
5. **Define Tone:** "compassionate", "patient-friendly", "evidence-based"

---

## üîê Prompt Security

### Current Safeguards:

- ‚úÖ User input is sanitized (validated UUID, escaped in SQL)
- ‚úÖ Prompt injection prevented by structured format
- ‚úÖ Output validated as JSON before storing
- ‚úÖ No user-generated content directly in prompts
- ‚úÖ Medical accuracy enforced through guidelines

### Potential Risks:

- ‚ö†Ô∏è If gene/mutation fields accept free text ‚Üí SQL injection risk (currently mitigated)
- ‚ö†Ô∏è LLM hallucination ‚Üí JSON parsing catches malformed output
- ‚ö†Ô∏è Medical misinformation ‚Üí Guidelines emphasize evidence-based info

---

## üìä Prompt Performance Metrics

### Current Performance:

| Metric | Value |
|--------|-------|
| Prompt Length | ~700 tokens |
| Response Length | ~500-800 tokens |
| LLM Call Time | 5-10 seconds (first call) |
| Cache Hit Time | ~100ms (subsequent calls) |
| JSON Parse Success | 95%+ |
| Cache Duration | 7 days |

---

## üé® Prompt Templates Library

### Template 1: Basic Genetic Analysis (Current)
**Use Case:** Standard single-gene variant analysis  
**Output:** JSON with condition, risks, recommendations

### Template 2: Multi-Gene Panel (Future)
**Use Case:** Multiple variants across different genes  
**Output:** Prioritized risk assessment with combined implications

### Template 3: Family History Context (Future)
**Use Case:** Variant interpretation with family cancer history  
**Output:** Personalized risk with pedigree considerations

### Template 4: Pharmacogenomics (Future)
**Use Case:** Drug metabolism and response variants  
**Output:** Medication guidance and dosing recommendations

---

## üîç Debugging Prompts

### Check Actual Prompt Sent:

Look for this in backend logs:
```
[INFO] ü§ñ Calling custom LLM for condition analysis...
```

### Check LLM Response:

```
[INFO] ‚úÖ Custom LLM response received: 1308 chars
```

### Check JSON Parsing:

**Success:**
```
[INFO] ‚úÖ condition_analysis:success condition=Hereditary Breast Cancer
```

**Failure:**
```
[ERROR] ‚ùå Failed to parse LLM response as JSON: ...
[ERROR] Raw response (first 500 chars): ...
```

---

## üìû Need Help?

### Modifying Prompts:

1. **Small Changes:** Edit `app.py` directly
2. **Major Changes:** Test locally first with Docker
3. **Validation:** Ensure JSON output remains valid
4. **Deployment:** Push to Azure, test on production

### Testing New Prompts:

```bash
# Local testing
curl -X GET http://localhost:8081/condition-analysis/<user-uuid>

# Azure testing
curl -X GET https://gene-guide-backend.azurecontainerapps.io/condition-analysis/<user-uuid> \
  -H "Authorization: Bearer <jwt-token>"
```

---

## üìö Related Documentation

- `CACHING_EXPLAINED.md` - How prompt responses are cached
- `app.py` (lines 852-893) - Actual prompt implementation
- `FORM_PRE_POPULATION.md` - How user data flows to prompts
- Custom LLM documentation (external)

